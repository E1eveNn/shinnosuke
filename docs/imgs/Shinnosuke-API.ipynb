{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ->->->->->->->->->->->->->->->-> Welcome <-<-<-<-<-<-<-<-<-<-<-<-<-<-<-<-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![shinnosuke](https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=1784511497,119911411&fm=26&gp=0.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Contents</h1>\n",
    "<ol>\n",
    "    <li><h2><a href=\"#layer\">Layer</a></h2>\n",
    "        <ul>\n",
    "            <li><h3><a href=\"#input\">Input</a></h3></li>\n",
    "            <li><h3><a href=\"#dense\">Dense</a></h3></li>\n",
    "            <li><h3><a href=\"#flatten\">Flatten</a></h3></li>\n",
    "            <li><h3><a href=\"#pad\">ZeroPadding2D</a></h3></li>\n",
    "            <li><h3><a href=\"#conv\">Conv2D</a></h3></li>\n",
    "            <li><h3><a href=\"#maxpool\">MaxPooling2D</a></h3></li>\n",
    "            <li><h3><a href=\"#meanpool\">MeanPooling2D</a></h3></li>\n",
    "            <li><h3><a href=\"#act\">Activation</a></h3></li>\n",
    "            <li><h3><a href=\"#reshape\">Reshape</a></h3></li>\n",
    "            <li><h3><a href=\"#dropout\">Dropout</a></h3></li>\n",
    "            <li><h3><a href=\"#bn\">Batch Normalization</a></h3></li>\n",
    "            <li><h3><a href=\"#ln\">Layer Normalization</a></h3></li>\n",
    "            <li><h3><a href=\"#gn\">Group Normalization</a></h3></li>\n",
    "            <li><h3><a href=\"#embed\">Embedding</a></h3></li>\n",
    "            <li><h3><a href=\"#rnn\">SimpleRNN</a></h3></li>\n",
    "            <li><h3><a href=\"#lstm\">LSTM</a></h3></li>\n",
    "            <li><h3><a href=\"#timedist\">TimeDistributed</a></h3></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><div></div><h2><a href=\"#node\">Node</a></h2>\n",
    "        <ul>\n",
    "            <li><h3><a href=\"#variable\">Variable</a></h3></li>\n",
    "            <li><h3><a href=\"#constant\">Constant</a></h3></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><div></div><h2><a href=\"#example\">Example</a></h2>\n",
    "        <ul>\n",
    "            <li><h3><a href=\"#ex-cnn\">CNN</a></h3></li>\n",
    "            <li><h3><a href=\"#ex-lstm\">LSTM</a></h3></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"layer\">Layer</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"input\"><i>- Input(shape: Tuple,  value: ndarray = None, **kwargs)</i></div>\n",
    "\n",
    "+ shape: input data's shape, for example, (None, C, H, W) or (None, features).\n",
    "+ value: this layer's input and output tensor's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "Input                (None, 10, 5, 5)     0            \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.models import Model\n",
    "from shinnosuke.layers import Input\n",
    "\n",
    "X = Input(shape=(None, 10, 5, 5))\n",
    "model = Model(inputs=X, outputs=X)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"dense\"><i>- Dense(n_out: int, n_in: int = None, initializer='glorot_uniform', activation='linear', kernel_regularizer=None, **kwargs)</i></div>\n",
    "\n",
    "+ n_out: out feature numbers.\n",
    "\n",
    "+ n_in: in feature numbers.\n",
    "\n",
    "+ initializer: kernel and bias initialize method. see details in <a href='#Initializers'>Initializers</a>\n",
    "\n",
    "+ activation: activation function. see details in <a href='#Activations'>Activations</a>\n",
    "\n",
    "+ kernel_regularizer: not implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "dense1 (Dense)       (None, 100)          50100        \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "dense2 (Dense)       (None, 10)           1010         dense1         \n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 51110\n",
      "Trainable params: 51110\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.models import Sequential\n",
    "from shinnosuke.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_out=100, n_in=500, activation='relu', name='dense1'))  # must specify n_in if this is the first layer of network\n",
    "model.add(Dense(n_out=10, name='dense2'))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"flatten\"><i>- Flatten(out_dim: int = 2, **kwargs)</i></div>\n",
    "\n",
    "+ out_dim: after flatten, the output data's dimension. for example, input data's shape is (N, C, H, W), out_dim = 2 will convert output data's shape to (N, $C \\times H \\times W$) and out_dim = 3 will convert output data's shape to (N, C, $H \\times W$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "Input                (None, 10, 5, 8)     0            \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "Flatten              (None, 400)          0            Input          \n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.models import Model\n",
    "from shinnosuke.layers import Flatten\n",
    "from shinnosuke.layers import Input\n",
    "\n",
    "X_input = Input(shape=(None, 10, 5, 8))\n",
    "X = Flatten(out_dim=2)(X_input)\n",
    "model = Model(inputs=X_input, outputs=X)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"pad\"><i>- ZeroPadding2D(pad_size: Tuple, **kwargs)</i></div>\n",
    "\n",
    "+ pad_size: for example, (1, 1), which means pad input(N, C, H, W) to (N, C, H+2, W+2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "Input                (None, 10, 5, 5)     0            \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "ZeroPadding2D        (None, 10, 9, 9)     0            Input          \n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.models import Model\n",
    "from shinnosuke.layers import ZeroPadding2D\n",
    "from shinnosuke.layers import Input\n",
    "\n",
    "X_input = Input(shape=(None, 10, 5, 5))\n",
    "X = ZeroPadding2D(pad_size=(2, 2))(X_input)\n",
    "model = Model(inputs=X_input, outputs=X)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"conv\"><i>- Conv2D(filter_nums: int, filter_size: Tuple, input_shape: Tuple = None, stride: int = 1, padding: str = 'VALID', activation = 'linear',initializer = 'Normal', **kwargs)</i></div>\n",
    "\n",
    "+ filter_nums: filter's numbers.\n",
    "\n",
    "+ filter_size: filter's size. for example, (3, 3) or 3.\n",
    "\n",
    "+ input_shape: must specify if this is the first layer of network.\n",
    "\n",
    "+ stride: convolution stride.\n",
    "\n",
    "+ padding: 'SAME' or 'VALID', 'VALID' means no padding, 'SAME' means pad input to get the same output size as input.\n",
    "\n",
    "+ activation: activation function. see details in <a href='#Activations'>Activations</a>\n",
    "\n",
    "+ initializer: kernel and bias initialize method. see details in <a href='#Initializers'>Initializers</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "Input                (None, 3, 24, 24)    0            \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "Conv2D               (None, 16, 22, 22)   448          Input          \n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 448\n",
      "Trainable params: 448\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.models import Model\n",
    "from shinnosuke.layers import Conv2D\n",
    "from shinnosuke.layers import Input\n",
    "\n",
    "X_input = Input(shape=(None, 3, 24, 24))\n",
    "X = Conv2D(filter_nums=16, filter_size=(3, 3), stride=1, padding='VALID', activation='relu')(X_input)\n",
    "model = Model(inputs=X_input, outputs=X)\n",
    "model.compile(optimizer='sgd', loss='bce')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"maxpool\"><i>- MaxPooling2D(pool_size: Tuple, stride: int = None, **kwargs)</i></div>\n",
    "\n",
    "+ pool_size: pooling kernel size, for example (2, 2) means apply max pooling in every 2 x 2 area.\n",
    "+ stride: pooling stride."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"meanpool\"><i>- MeanPooling2D(pool_size: Tuple, stride: int = None, **kwargs)</i></div>\n",
    "\n",
    "+ pool_size: pooling kernel size, for example (2, 2) means apply mean pooling in every 2 x 2 area.\n",
    "+ stride: pooling stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "Input                (None, 3, 24, 24)    0            \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "MaxPooling2D         (None, 3, 12, 12)    0            Input          \n",
      "---------------------------------------------------------------------------\n",
      "MeanPooling2D        (None, 3, 6, 6)      0            MaxPooling2D   \n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.models import Model\n",
    "from shinnosuke.layers import MaxPooling2D, MeanPooling2D\n",
    "from shinnosuke.layers import Input\n",
    "\n",
    "X_input = Input(shape=(None, 3, 24, 24))\n",
    "X = MaxPooling2D(pool_size=(2, 2))(X_input)\n",
    "X = MeanPooling2D(pool_size=(2, 2))(X)\n",
    "model = Model(inputs=X_input, outputs=X)\n",
    "model.compile(optimizer='sgd', loss='bce')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"act\"><i>- Activation(act_name='relu')</i></div>\n",
    "\n",
    "+ act_name: activation function name, support ReLU, Sigmoid, etc. see details in <a href='#Activations'>Activations</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "Input                (None, 3, 24, 24)    0            \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "Activation           (None, 3, 24, 24)    0            Input          \n",
      "---------------------------------------------------------------------------\n",
      "Activation           (None, 3, 24, 24)    0            Activation     \n",
      "---------------------------------------------------------------------------\n",
      "Activation           (None, 3, 24, 24)    0            Activation     \n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.models import Model\n",
    "from shinnosuke.layers import Activation\n",
    "from shinnosuke.layers import Input\n",
    "\n",
    "X_input = Input(shape=(None, 3, 24, 24))\n",
    "X = Activation('relu')(X_input)\n",
    "X = Activation('sigmoid')(X)\n",
    "X = Activation('softmax')(X)\n",
    "model = Model(inputs=X_input, outputs=X)\n",
    "model.compile(optimizer='sgd', loss='cross_entropy')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"reshape\"><i>- Reshape(output_shape: Tuple, **kwargs)</i></div>\n",
    "\n",
    "+ output_shape: shape after reshape operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "Input                (None, 3, 5, 4)      0            \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "Reshape              (None, 12, 5)        0            Input          \n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.models import Model\n",
    "from shinnosuke.layers import Reshape\n",
    "from shinnosuke.layers import Input\n",
    "\n",
    "X_input = Input(shape=(None, 3, 5, 4))\n",
    "X = Reshape(output_shape=(None, 12, 5))(X_input)\n",
    "model = Model(inputs=X_input, outputs=X)\n",
    "model.compile(optimizer='sgd', loss='cross_entropy')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"dropout\"><i>- Dropout(keep_prob)</i></div>\n",
    "\n",
    "+ keep_prob:  probability of keeping a unit active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "Input                (None, 500)          0            \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "Dropout              (None, 500)          0            Input          \n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.models import Model\n",
    "from shinnosuke.layers import Dropout\n",
    "from shinnosuke.layers import Input\n",
    "\n",
    "X_input = Input(shape=(None, 500))\n",
    "X = Dropout(keep_prob=0.5)(X_input)\n",
    "model = Model(inputs=X_input, outputs=X)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"bn\"><i>- Batch Normalization(epsilon=1e-6, momentum=0.9, axis=1, gamma_initializer='ones', beta_initializer='zeros', moving_mean_initializer='zeros', moving_variance_initializer='ones')</i></div>\n",
    "$$\n",
    "u_B = \\frac{1}{m} \\sum \\limits_{i=1}^m x_i  \\quad \\quad mini-batch \\quad mean\n",
    "\\\\\n",
    "\\sigma_B = \\frac{1}{m} \\sum \\limits_{i=1}^m (x_i - u_B)^2  \\quad \\quad mini-batch \\quad variance\n",
    "\\\\\n",
    "\\hat x_i = \\frac{x_i - u_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}   \\quad \\quad normalize\n",
    "\\\\\n",
    "y_i = \\gamma \\hat x_i + \\beta  \\quad \\quad scale \\quad and \\quad shift\n",
    "$$\n",
    "\n",
    "\n",
    "+ epsilon:  $\\epsilon$ value.\n",
    "+ momentum: at training time, we use moving averages to update $u_B \\rightarrow$ $moving\\_u = momentum * moving\\_u + (1 - momentum) * u_B$ and $\\sigma_B \\rightarrow  moving\\_\\sigma = momentum * moving\\_\\sigma + (1 - momentum) * \\sigma_B$ \n",
    "+ axis: use normalization on which axis, for Dense Layer, it should be 1 or -1, for Convolution Layer, it should be 1.\n",
    "+ gamma_initializer: initialize $\\gamma$ method. see details in <a href='#Initializers'>Initializers</a>\n",
    "+ beta_initializer: initialize $\\beta$ method. see details in <a href='#Initializers'>Initializers</a>\n",
    "+ moving_mean_initializer: initialize $moving\\_u$ method. see details in <a href='#Initializers'>Initializers</a>\n",
    "+ moving_variance_initializer: initialize $moving\\_\\sigma$ method. see details in <a href='#Initializers'>Initializers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"ln\"><i>- Layer Normalization(epsilon=1e-10, gamma_initializer='ones', beta_initializer='zeros')</i></div>\n",
    "$$\n",
    "u = \\frac{1}{CHW} \\sum \\limits_{i=1}^C \\sum \\limits_{j=1}^H \\sum \\limits_{k=1}^W x_{ijk}  \\quad \\quad sample \\quad mean\n",
    "\\\\\n",
    "\\sigma = \\frac{1}{CHW} \\sum \\limits_{i=1}^C \\sum \\limits_{j=1}^H \\sum \\limits_{k=1}^W (x_{ijk} - u)^2  \\quad \\quad sample \\quad variance\n",
    "\\\\\n",
    "\\hat x = \\frac{x - u}{\\sqrt{\\sigma^2 + \\epsilon}}   \\quad \\quad normalize\n",
    "\\\\\n",
    "y = \\gamma \\hat x + \\beta  \\quad \\quad scale \\quad and \\quad shift\n",
    "$$\n",
    "\n",
    "\n",
    "+ epsilon:  $\\epsilon$ value.\n",
    "+ gamma_initializer: initialize $\\gamma$ method. see details in <a href='#Initializers'>Initializers</a>\n",
    "+ beta_initializer: initialize $\\beta$ method. see details in <a href='#Initializers'>Initializers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"gn\"><i>- Group Normalization(epsilon=1e-5, G=16,gamma_initializer='ones', beta_initializer='zeros')</i></div>\n",
    "split channel into G groups, for each group, applying layer normalization separately.\n",
    "$$\n",
    "\\\\\n",
    "u = \\frac{1}{CHW} \\sum \\limits_{i=1}^C \\sum \\limits_{j=1}^H \\sum \\limits_{k=1}^W x_{ijk}  \\quad \\quad sample \\quad mean\n",
    "\\\\\n",
    "\\sigma = \\frac{1}{CHW} \\sum \\limits_{i=1}^C \\sum \\limits_{j=1}^H \\sum \\limits_{k=1}^W (x_{ijk} - u)^2  \\quad \\quad sample \\quad variance\n",
    "\\\\\n",
    "\\hat x = \\frac{x - u}{\\sqrt{\\sigma^2 + \\epsilon}}   \\quad \\quad normalize\n",
    "\\\\\n",
    "y = \\gamma \\hat x + \\beta  \\quad \\quad scale \\quad and \\quad shift\n",
    "$$\n",
    "\n",
    "\n",
    "+ epsilon:  $\\epsilon$ value.\n",
    "+ G: group numbers.\n",
    "+ gamma_initializer: initialize $\\gamma$ method. see details in <a href='#Initializers'>Initializers</a>\n",
    "+ beta_initializer: initialize $\\beta$ method. see details in <a href='#Initializers'>Initializers</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "Input                (None, 16, 5, 5)     0            \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "BatchNormalization   (None, 16, 5, 5)     32           Input          \n",
      "---------------------------------------------------------------------------\n",
      "LayerNormalization   (None, 16, 5, 5)     800          BatchNormalization\n",
      "---------------------------------------------------------------------------\n",
      "GroupNormalization   (None, 16, 5, 5)     32           LayerNormalization\n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 864\n",
      "Trainable params: 864\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.models import Model\n",
    "from shinnosuke.layers import BatchNormalization, LayerNormalization, GroupNormalization\n",
    "from shinnosuke.layers import Input\n",
    "\n",
    "X_input = Input(shape=(None, 16, 5, 5))\n",
    "X = BatchNormalization()(X_input)\n",
    "X = LayerNormalization()(X)\n",
    "X = GroupNormalization()(X)\n",
    "model = Model(inputs=X_input, outputs=X)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"embed\"><i>- Embedding(input_dim, output_dim,embeddings_initializer='uniform', mask_zero=False, input_length=None, **kwargs)</i></div>\n",
    "\n",
    "+ input_dim: to embedded dimension, for example, input data shape is (N, T), input_dim is the max value.\n",
    "+ out_dim: after embedding dimension, for example, out_dim = E, input data (N, T) after embedding's shape is (N, T, E).\n",
    "+ embeddings_initializer: embedding kernel initialize method. see details in <a href='#Initializers'>Initializers</a>\n",
    "+ mask_zero: use masks.\n",
    "+ input_length: must specify if this layer is first layer of network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "Embedding            (None, 30, 200)      1000000      \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 1000000\n",
      "Trainable params: 1000000\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.models import Sequential\n",
    "from shinnosuke.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=200, input_length=30))\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"rnn\"><i>- SimpleRNN(units, activation='tanh', initializer='glorotuniform', recurrent_initializer='orthogonal', return_sequences=False, return_state=False, stateful=False, **kwargs)</i></div>\n",
    "\n",
    "$$\n",
    "z^t = W_{aa}\\cdot a^{t-1} + W_{xa}\\cdot x^t +b_a\n",
    "\\\\\n",
    "a^t = activation(z^t)\n",
    "$$\n",
    "\n",
    "+ units: rnn hidden unit numbers, for example, units = a, input data (N, T, L) after rnn will output (N, T, a).\n",
    "+ activation: activation method. see details in <a href='#Activations'>Activations</a>\n",
    "+ initializer: $W_{xa}$ initialize method. see details in <a href='#Initializers'>Initializers</a>\n",
    "+ recurrent_initializer: $W_{aa}$ initialize method. see details in <a href='#Initializers'>Initializers</a>\n",
    "+ return_sequences: if True, return all timesteps a $\\rightarrow$ $[a^1, a^2,..., a^t]$; if False, return the last timesteps $a^t$.\n",
    "+ return_state: if True, return return_sequences' result and all timesteps a.\n",
    "+ stateful: if True, use last time $a^t$ to initialize this time $a^1$; if False, use 0 to initialize this time $a^1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "Input                (None, 30, 200)      0            \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "SimpleRNN            (None, 50)           12550        Input          \n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 12550\n",
      "Trainable params: 12550\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.models import Model\n",
    "from shinnosuke.layers import SimpleRNN\n",
    "from shinnosuke.layers import Input\n",
    "\n",
    "X_input = Input(shape=(None, 30, 200))\n",
    "X = SimpleRNN(units=50)(X_input)\n",
    "model = Model(inputs=X_input, outputs=X)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"lstm\"><i>- LSTM(units, activation='tanh', recurrent_activation='sigmoid', initializer='glorotuniform', recurrent_initializer='orthogonal', unit_forget_bias=True, return_sequences=False, return_state=False, stateful=False, **kwargs)</i></div>\n",
    "\n",
    "at every timesteps\n",
    "\n",
    "$$\n",
    "i^t = recurrent\\_activation(W_i[a^{t-1}, x^t] + b_i)\n",
    "\\\\\n",
    "f^t = recurrent\\_activation(W_f[a^{t-1}, x^t] + b_f)\n",
    "\\\\\n",
    "\\tilde c^t = activation(W_c[a^{t-1}, x^t] + b_c)\n",
    "\\\\\n",
    "c^t = f^t \\cdot c^{t-1} + i^t \\cdot \\tilde c^t\n",
    "\\\\\n",
    "o^t = recurrent\\_activation(W_o[a^{t-1}, x^t] + b_o)\n",
    "\\\\\n",
    "a^t = o^t \\cdot tanh(c^t)\n",
    "$$\n",
    "\n",
    "+ units: lstm hidden unit numbers.\n",
    "+ activation: activation method. see details in <a href='#Activations'>Activations</a>\n",
    "+ recurrent_activation: activation method. see details in <a href='#Activations'>Activations</a>\n",
    "+ initializer: $W_{xa}$ initialize method. see details in <a href='#Initializers'>Initializers</a>\n",
    "+ recurrent_initializer: $W_{aa}$ initialize method. see details in <a href='#Initializers'>Initializers</a>\n",
    "+ unit_forget_bias: if True, initialize $f^t$ bias $b_f$ as 1, else 0.\n",
    "+ return_sequences: if True, return all timesteps a $\\rightarrow$ $[a^1, a^2,..., a^t]$; if False, return the last timesteps $a^t$.\n",
    "+ return_state: if True, return return_sequences' result and all timesteps a.\n",
    "+ stateful: if True, use last time $a^t$ to initialize this time $a^1$; if False, use 0 to initialize this time $a^1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "Input                (None, 30, 200)      0            \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "LSTM                 (None, 30, 50)       50200        Input          \n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 50200\n",
      "Trainable params: 50200\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.models import Model\n",
    "from shinnosuke.layers import LSTM\n",
    "from shinnosuke.layers import Input\n",
    "\n",
    "X_input = Input(shape=(None, 30, 200))\n",
    "X = LSTM(units=50, return_sequences=True)(X_input)\n",
    "model = Model(inputs=X_input, outputs=X)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"timedist\"><i>- TimeDistributed(layer, **kwargs)</i></div>\n",
    "\n",
    "+ layer: to apply time distributed layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "Input                (None, 25, 97)       0            \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "LSTM                 (None, 25, 100)      79200        Input          \n",
      "---------------------------------------------------------------------------\n",
      "TimeDistributed      (None, 25, 50)       5050         LSTM           \n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 84250\n",
      "Trainable params: 84250\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.layers import Input, Dense, LSTM, TimeDistributed\n",
    "from shinnosuke.models import Model\n",
    "\n",
    "X_input = Input(shape=(None, 25, 97))\n",
    "X = LSTM(units=100, return_sequences=True, stateful=True)(X_input)\n",
    "X = TimeDistributed(Dense(50))(X)\n",
    "model = Model(inputs=X_input, outputs=X)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"node\">Node</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"variable\"><i>- Variable(initial_value=None, shape=None,name='variable')</i></div>\n",
    "\n",
    "+ initial_value: initialize value of this variable.\n",
    "+ shape: this variable's shape.\n",
    "+ name: this variable's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before change:  3\n",
      "after change:  2\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke import Variable\n",
    "\n",
    "a = Variable(initial_value=3)\n",
    "print('before change: ', a)\n",
    "# change variable's value\n",
    "a.output_tensor = 2\n",
    "print('after change: ',a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: red\" id=\"constant\"><i>- Constant(output_tensor, name='constant')</i></div>\n",
    "\n",
    "+ output_tensor: this constant value, once initialized, it can't be changed.\n",
    "+ name: this constant's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before change:  3\n",
      "after change:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\envs\\ml\\lib\\site-packages\\shinnosuke\\layers\\Core.py:107: UserWarning: Can not change Constant value!\n",
      "  warnings.warn('Can not change Constant value!')\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke import Constant\n",
    "\n",
    "a = Constant(3)\n",
    "print('before change: ', a)\n",
    "# change variable's value\n",
    "a.output_tensor = 2\n",
    "print('after change: ',a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"example\">Example</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"ex-cnn\">CNN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "Input                (None, 3, 64, 64)    0            \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "Conv2D               (None, 8, 64, 64)    224          Input          \n",
      "---------------------------------------------------------------------------\n",
      "BatchNormalization   (None, 8, 64, 64)    16           Conv2D         \n",
      "---------------------------------------------------------------------------\n",
      "Activation           (None, 8, 64, 64)    0            BatchNormalization\n",
      "---------------------------------------------------------------------------\n",
      "MaxPooling2D         (None, 8, 32, 32)    0            Activation     \n",
      "---------------------------------------------------------------------------\n",
      "Conv2D               (None, 16, 32, 32)   1168         MaxPooling2D   \n",
      "---------------------------------------------------------------------------\n",
      "BatchNormalization   (None, 16, 32, 32)   32           Conv2D         \n",
      "---------------------------------------------------------------------------\n",
      "Activation           (None, 16, 32, 32)   0            BatchNormalization\n",
      "---------------------------------------------------------------------------\n",
      "MaxPooling2D         (None, 16, 16, 16)   0            Activation     \n",
      "---------------------------------------------------------------------------\n",
      "Flatten              (None, 4096)         0            MaxPooling2D   \n",
      "---------------------------------------------------------------------------\n",
      "Dense                (None, 1000)         4097000      Flatten        \n",
      "---------------------------------------------------------------------------\n",
      "Dropout              (None, 1000)         0            Dense          \n",
      "---------------------------------------------------------------------------\n",
      "Dense                (None, 500)          500500       Dropout        \n",
      "---------------------------------------------------------------------------\n",
      "Dropout              (None, 500)          0            Dense          \n",
      "---------------------------------------------------------------------------\n",
      "Dense                (None, 10)           5010         Dropout        \n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 4603950\n",
      "Trainable params: 4603950\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization, Activation, Input\n",
    "from shinnosuke.models import Model\n",
    "from shinnosuke.utils import StochasticGradientDescent\n",
    "# define model\n",
    "def create_model():\n",
    "    X_input = Input(shape=(None, 3, 64, 64))\n",
    "    X = Conv2D(filter_nums=8, filter_size=(3, 3), padding='SAME')(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Conv2D(filter_nums=16, filter_size=(3, 3), padding='SAME')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(n_out=1000, activation='relu')(X)\n",
    "    X = Dropout(keep_prob=0.8)(X)\n",
    "    X = Dense(n_out=500, activation='relu')(X)\n",
    "    X = Dropout(keep_prob=0.8)(X)\n",
    "    X = Dense(n_out=10, activation='softmax')(X)\n",
    "    model = Model(inputs=X_input, outputs=X)\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "sgd = StochasticGradientDescent(lr=0.01)\n",
    "model.compile(loss='SparseCategoricalCrossEntropy', optimizer=sgd)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m Epoch[1/20]\n",
      "100/100 [==============================>] -657ms -328ms/batch -batch_loss: 11.6892 -batch_acc: 0.0556 \n",
      "\u001b[0;31m Epoch[2/20]\n",
      "100/100 [==============================>] -646ms -323ms/batch -batch_loss: 9.5247 -batch_acc: 0.1111 \n",
      "\u001b[0;31m Epoch[3/20]\n",
      "100/100 [==============================>] -638ms -319ms/batch -batch_loss: 7.4662 -batch_acc: 0.0556 \n",
      "\u001b[0;31m Epoch[4/20]\n",
      "100/100 [==============================>] -622ms -311ms/batch -batch_loss: 2.7718 -batch_acc: 0.1944 \n",
      "\u001b[0;31m Epoch[5/20]\n",
      "100/100 [==============================>] -636ms -318ms/batch -batch_loss: 2.3277 -batch_acc: 0.1389 \n",
      "\u001b[0;31m Epoch[6/20]\n",
      "100/100 [==============================>] -560ms -280ms/batch -batch_loss: 1.7361 -batch_acc: 0.4722 \n",
      "\u001b[0;31m Epoch[7/20]\n",
      "100/100 [==============================>] -494ms -247ms/batch -batch_loss: 1.5181 -batch_acc: 0.5278 \n",
      "\u001b[0;31m Epoch[8/20]\n",
      "100/100 [==============================>] -482ms -241ms/batch -batch_loss: 1.5116 -batch_acc: 0.4167 \n",
      "\u001b[0;31m Epoch[9/20]\n",
      "100/100 [==============================>] -469ms -234ms/batch -batch_loss: 1.2294 -batch_acc: 0.6111 \n",
      "\u001b[0;31m Epoch[10/20]\n",
      "100/100 [==============================>] -447ms -223ms/batch -batch_loss: 1.1452 -batch_acc: 0.6667 \n",
      "\u001b[0;31m Epoch[11/20]\n",
      "100/100 [==============================>] -416ms -208ms/batch -batch_loss: 1.0528 -batch_acc: 0.7778 \n",
      "\u001b[0;31m Epoch[12/20]\n",
      "100/100 [==============================>] -414ms -207ms/batch -batch_loss: 1.0397 -batch_acc: 0.7222 \n",
      "\u001b[0;31m Epoch[13/20]\n",
      "100/100 [==============================>] -403ms -201ms/batch -batch_loss: 0.7893 -batch_acc: 0.8056 \n",
      "\u001b[0;31m Epoch[14/20]\n",
      "100/100 [==============================>] -401ms -200ms/batch -batch_loss: 0.6703 -batch_acc: 0.8889 \n",
      "\u001b[0;31m Epoch[15/20]\n",
      "100/100 [==============================>] -602ms -301ms/batch -batch_loss: 0.5584 -batch_acc: 0.9444 \n",
      "\u001b[0;31m Epoch[16/20]\n",
      "100/100 [==============================>] -661ms -330ms/batch -batch_loss: 0.4782 -batch_acc: 0.9722 \n",
      "\u001b[0;31m Epoch[17/20]\n",
      "100/100 [==============================>] -669ms -334ms/batch -batch_loss: 0.3876 -batch_acc: 0.9444 \n",
      "\u001b[0;31m Epoch[18/20]\n",
      "100/100 [==============================>] -626ms -313ms/batch -batch_loss: 0.4167 -batch_acc: 0.9722 \n",
      "\u001b[0;31m Epoch[19/20]\n",
      "100/100 [==============================>] -645ms -322ms/batch -batch_loss: 0.3707 -batch_acc: 1.0000 \n",
      "\u001b[0;31m Epoch[20/20]\n",
      "100/100 [==============================>] -632ms -316ms/batch -batch_loss: 0.2052 -batch_acc: 1.0000 \n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "from shinnosuke.utils import to_categorical\n",
    "# randomly generate training datas\n",
    "cp.random.seed(1)\n",
    "X = cp.random.random((100, 3, 64, 64))\n",
    "y = cp.random.randint(0, 10, size=(100, 1))\n",
    "# to one-hot\n",
    "y = to_categorical(y)\n",
    "\n",
    "model.fit(X, y, batch_size=64, epochs=20, validation_ratio=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"ex-lstm\">LSTM</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************************\n",
      "Layer(type)          Output Shape         Param        Connected to   \n",
      "###########################################################################\n",
      "Input                (None, 20)           0            \n",
      "              \n",
      "---------------------------------------------------------------------------\n",
      "Embedding            (None, 20, 100)      50000        Input          \n",
      "---------------------------------------------------------------------------\n",
      "LSTM                 (None, 20, 100)      80400        Embedding      \n",
      "---------------------------------------------------------------------------\n",
      "TimeDistributed      (None, 20, 50)       5050         LSTM           \n",
      "---------------------------------------------------------------------------\n",
      "LSTM                 (None, 50)           20200        TimeDistributed\n",
      "---------------------------------------------------------------------------\n",
      "Dense                (None, 1)            51           LSTM           \n",
      "---------------------------------------------------------------------------\n",
      "Activation           (None, 1)            0            Dense          \n",
      "---------------------------------------------------------------------------\n",
      "***************************************************************************\n",
      "Total params: 155701\n",
      "Trainable params: 155701\n",
      "Non-trainable params: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shinnosuke.models import Model\n",
    "from shinnosuke.layers import Input, LSTM, TimeDistributed, Dense, Activation, Embedding\n",
    "from shinnosuke.utils import StochasticGradientDescent\n",
    "\n",
    "def create_model():\n",
    "    X_input = Input(shape=(None, 20))\n",
    "    X = Embedding(input_dim=500, output_dim=100)(X_input)\n",
    "    X = LSTM(units=100, return_sequences=True)(X)\n",
    "    X = TimeDistributed(Dense(n_out=50))(X)\n",
    "    X = LSTM(units=50)(X)\n",
    "    X = Dense(n_out=1)(X)\n",
    "    X = Activation('sigmoid')(X)\n",
    "    model = Model(inputs=X_input, outputs=X)\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "sgd = StochasticGradientDescent(lr=0.5)\n",
    "model.compile(optimizer=sgd, loss='binary_cross_entropy')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m Epoch[1/20]\n",
      "100/100 [==============================>] -709ms -354ms/batch -batch_loss: 0.6986 -batch_acc: 0.3800 \n",
      "\u001b[0;31m Epoch[2/20]\n",
      "100/100 [==============================>] -715ms -357ms/batch -batch_loss: 0.6853 -batch_acc: 0.5800 \n",
      "\u001b[0;31m Epoch[3/20]\n",
      "100/100 [==============================>] -710ms -355ms/batch -batch_loss: 0.6889 -batch_acc: 0.5600 \n",
      "\u001b[0;31m Epoch[4/20]\n",
      "100/100 [==============================>] -715ms -357ms/batch -batch_loss: 0.7001 -batch_acc: 0.5000 \n",
      "\u001b[0;31m Epoch[5/20]\n",
      "100/100 [==============================>] -739ms -369ms/batch -batch_loss: 0.6869 -batch_acc: 0.6200 \n",
      "\u001b[0;31m Epoch[6/20]\n",
      "100/100 [==============================>] -718ms -359ms/batch -batch_loss: 0.6982 -batch_acc: 0.5200 \n",
      "\u001b[0;31m Epoch[7/20]\n",
      "100/100 [==============================>] -711ms -355ms/batch -batch_loss: 0.6947 -batch_acc: 0.5400 \n",
      "\u001b[0;31m Epoch[8/20]\n",
      "100/100 [==============================>] -720ms -360ms/batch -batch_loss: 0.6945 -batch_acc: 0.5400 \n",
      "\u001b[0;31m Epoch[9/20]\n",
      "100/100 [==============================>] -712ms -356ms/batch -batch_loss: 0.6969 -batch_acc: 0.5200 \n",
      "\u001b[0;31m Epoch[10/20]\n",
      "100/100 [==============================>] -735ms -367ms/batch -batch_loss: 0.6924 -batch_acc: 0.5600 \n",
      "\u001b[0;31m Epoch[11/20]\n",
      "100/100 [==============================>] -719ms -359ms/batch -batch_loss: 0.6961 -batch_acc: 0.5200 \n",
      "\u001b[0;31m Epoch[12/20]\n",
      "100/100 [==============================>] -717ms -358ms/batch -batch_loss: 0.6858 -batch_acc: 0.5800 \n",
      "\u001b[0;31m Epoch[13/20]\n",
      "100/100 [==============================>] -795ms -397ms/batch -batch_loss: 0.7040 -batch_acc: 0.5000 \n",
      "\u001b[0;31m Epoch[14/20]\n",
      "100/100 [==============================>] -730ms -365ms/batch -batch_loss: 0.6889 -batch_acc: 0.5600 \n",
      "\u001b[0;31m Epoch[15/20]\n",
      "100/100 [==============================>] -720ms -360ms/batch -batch_loss: 0.7038 -batch_acc: 0.5000 \n",
      "\u001b[0;31m Epoch[16/20]\n",
      "100/100 [==============================>] -719ms -359ms/batch -batch_loss: 0.6974 -batch_acc: 0.5200 \n",
      "\u001b[0;31m Epoch[17/20]\n",
      "100/100 [==============================>] -716ms -358ms/batch -batch_loss: 0.6849 -batch_acc: 0.5800 \n",
      "\u001b[0;31m Epoch[18/20]\n",
      "100/100 [==============================>] -718ms -359ms/batch -batch_loss: 0.6847 -batch_acc: 0.6000 \n",
      "\u001b[0;31m Epoch[19/20]\n",
      "100/100 [==============================>] -728ms -364ms/batch -batch_loss: 0.6884 -batch_acc: 0.5600 \n",
      "\u001b[0;31m Epoch[20/20]\n",
      "100/100 [==============================>] -717ms -358ms/batch -batch_loss: 0.6843 -batch_acc: 0.5800 \n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "from shinnosuke.utils import to_categorical\n",
    "\n",
    "cp.random.seed(1)\n",
    "X = cp.random.randint(0, 500, size=(100, 20))  # (N, timestep)\n",
    "y = cp.random.randint(0, 2, size=(100, 1))\n",
    "\n",
    "\n",
    "model.fit(X, y, batch_size=50, epochs=20, validation_ratio=0.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
